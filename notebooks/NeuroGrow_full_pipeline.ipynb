{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe8dec4d-e961-4167-934a-52507ad55fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d0861a7-26ad-4f28-bba5-7d6d85679a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset, random_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a9ddeb5-7eaa-4e24-baed-c39b6972a5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f7515b-5134-4551-a07e-e71ae545e739",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"../APPLESEED_Dataset\"  # adjust path if needed\n",
    "\n",
    "def load_sub_session(sub_path, ses):\n",
    "    eeg_path = os.path.join(sub_path, f\"ses-{ses}\", \"eeg\")\n",
    "    vhdr_files = [f for f in os.listdir(eeg_path) if f.endswith(\".vhdr\")]\n",
    "    raws = []\n",
    "    for vhdr in vhdr_files:\n",
    "        raw = mne.io.read_raw_brainvision(os.path.join(eeg_path, vhdr), preload=True)\n",
    "        raw.filter(1., 40.)  # bandpass filter\n",
    "        raw.set_eeg_reference('average')\n",
    "        raws.append(raw)\n",
    "    return raws\n",
    "\n",
    "def load_all_subjects(dataset_path):\n",
    "    subjects = [d for d in os.listdir(dataset_path) if d.startswith(\"sub\")]\n",
    "    all_data = []\n",
    "    labels = []\n",
    "    for sub in subjects:\n",
    "        sub_path = os.path.join(dataset_path, sub)\n",
    "        for ses in [\"1\",\"2\",\"3\",\"4\"]:\n",
    "            try:\n",
    "                raws = load_sub_session(sub_path, ses)\n",
    "                all_data.extend(raws)\n",
    "                # Example: session number as label (4,8,12,16 weeks)\n",
    "                labels.extend([int(ses)]*len(raws))\n",
    "            except:\n",
    "                continue\n",
    "    return all_data, np.array(labels)\n",
    "\n",
    "print(\"Loading EEG data...\")\n",
    "all_raws, labels = load_all_subjects(dataset_path)\n",
    "print(f\"Total EEG recordings: {len(all_raws)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f87f60-4c3e-49c0-a40f-8b0699bfe73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266a9e70-4070-48bc-9027-b73ce4e4483a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(raw):\n",
    "    psd, freqs = psd_welch(raw, fmin=1, fmax=40, n_fft=256)\n",
    "    psd = np.log(psd + 1e-6)\n",
    "    return psd\n",
    "\n",
    "print(\"Extracting features...\")\n",
    "X = np.array([extract_features(raw) for raw in all_raws])\n",
    "print(\"Feature array shape:\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0723bb99-75fd-4b53-8e3f-c4df9688411e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supervised Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50d9cb3-b517-4c2f-a111-2e0949687b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "train_size = int(0.8*len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b13b83-b1d6-42ce-babb-84b3ccc9a6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN+LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9706a4f2-650f-4617-9904-7a14676f60ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_LSTM(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(n_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "        self.lstm = nn.LSTM(128, 128, batch_first=True)\n",
    "        self.fc = nn.Linear(128, n_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = x.permute(0,2,1)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :]\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1a06b7-8fc7-40b0-8a24-30c255adcd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448d5627-0cc8-4bde-a1e8-4e58b6f77612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "n_channels = X_tensor.shape[1] if len(X_tensor.shape) > 2 else 1\n",
    "n_classes = len(torch.unique(y_tensor))\n",
    "model = CNN_LSTM(n_channels, n_classes).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 30\n",
    "train_losses, val_losses, val_accs = [], [], []\n",
    "\n",
    "print(\" Training started...\\n\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_val, y_val in val_loader:\n",
    "            X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "            outputs = model(X_val)\n",
    "            loss = criterion(outputs, y_val)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += y_val.size(0)\n",
    "            correct += (predicted == y_val).sum().item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_acc = correct / total\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f} | \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f} | \"\n",
    "          f\"Val Acc: {val_acc*100:.2f}%\")\n",
    "\n",
    "print(\"\\n Training complete!\")\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), \"models/neurogrow_cnn_lstm.pth\")\n",
    "print(\"Model saved to models/neurogrow_cnn_lstm.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d32c65c-91ef-492c-ae6d-de36f4dbcc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization - Supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972abf27-20ef-436e-b2bd-c42dc734a7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Val Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Loss Curves\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(val_accs, label=\"Validation Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Validation Accuracy\")\n",
    "plt.show()\n",
    "\n",
    "raw = all_raws[0]\n",
    "psd, freqs = psd_welch(raw)\n",
    "mne.viz.plot_topomap(np.mean(psd, axis=1), pos=raw.info, show=True, names=raw.ch_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2476e05b-0528-4cc8-abbb-6b7934b5786d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsupervised Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec65731-c301-4f8b-8a08-4d51f68feac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare flattened data for autoencoder\n",
    "X_flat = X.reshape(len(X), -1)                      # shape: (n_samples, n_channels * n_freq_bins)\n",
    "X_tensor_flat = torch.tensor(X_flat, dtype=torch.float32)\n",
    "\n",
    "# Dataset & DataLoader\n",
    "batch_size_ae = 8\n",
    "dataset_flat = TensorDataset(X_tensor_flat)\n",
    "loader_flat = DataLoader(dataset_flat, batch_size=batch_size_ae, shuffle=True)\n",
    "\n",
    "# Autoencoder model\n",
    "class EEG_Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim=64):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, latent_dim)\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        recon = self.decoder(z)\n",
    "        return recon, z\n",
    "\n",
    "input_dim = X_flat.shape[1]\n",
    "latent_dim = 64\n",
    "ae_model = EEG_Autoencoder(input_dim=input_dim, latent_dim=latent_dim).to(device)\n",
    "\n",
    "# Training setup\n",
    "optimizer_ae = torch.optim.Adam(ae_model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "criterion_ae = nn.MSELoss()\n",
    "\n",
    "ae_epochs = 30\n",
    "ae_losses = []\n",
    "\n",
    "os.makedirs(\"../models\", exist_ok=True)\n",
    "os.makedirs(\"../results\", exist_ok=True)\n",
    "\n",
    "print(\"üîÅ Starting autoencoder training...\")\n",
    "for epoch in range(ae_epochs):\n",
    "    ae_model.train()\n",
    "    epoch_loss = 0.0\n",
    "    nbatches = 0\n",
    "    for batch in loader_flat:\n",
    "        xb = batch[0].to(device)          # shape: (B, input_dim)\n",
    "        optimizer_ae.zero_grad()\n",
    "        recon, _ = ae_model(xb)\n",
    "        loss = criterion_ae(recon, xb)\n",
    "        loss.backward()\n",
    "        optimizer_ae.step()\n",
    "        epoch_loss += loss.item()\n",
    "        nbatches += 1\n",
    "    avg_loss = epoch_loss / nbatches\n",
    "    ae_losses.append(avg_loss)\n",
    "    print(f\"AE Epoch [{epoch+1}/{ae_epochs}] - Loss: {avg_loss:.6f}\")\n",
    "\n",
    "# Save autoencoder\n",
    "ae_path = \"../models/autoencoder_eeg.pth\"\n",
    "torch.save(ae_model.state_dict(), ae_path)\n",
    "print(f\" Autoencoder saved to {ae_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a38c88-7082-4fd5-bf40-31cf793955fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce352105-5e52-4392-b725-b8da9dc6b6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsupervised Visualizations (full)\n",
    "\n",
    "# 1) Plot autoencoder training loss curve\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(1, len(ae_losses)+1), ae_losses, marker='o')\n",
    "plt.title(\"Autoencoder Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../results/autoencoder_loss.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# 2) Obtain latent vectors for all samples (batch inference)\n",
    "ae_model.eval()\n",
    "with torch.no_grad():\n",
    "    X_tensor_flat_device = X_tensor_flat.to(device)\n",
    "    _, latent_vectors = ae_model(X_tensor_flat_device)   # shape: (n_samples, latent_dim)\n",
    "latent = latent_vectors.cpu().numpy()\n",
    "\n",
    "# 3) t-SNE visualization of latent space\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
    "latent_2d = tsne.fit_transform(latent)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "scatter = plt.scatter(latent_2d[:,0], latent_2d[:,1], c=labels, cmap=\"tab10\", s=25, alpha=0.85)\n",
    "plt.title(\"t-SNE of Autoencoder Latent Space\")\n",
    "plt.xlabel(\"t-SNE 1\")\n",
    "plt.ylabel(\"t-SNE 2\")\n",
    "plt.colorbar(scatter, label=\"Session label (example)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../results/ae_tsne.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# 4) PCA visualization (complementary)\n",
    "pca = PCA(n_components=2)\n",
    "pca_2d = pca.fit_transform(latent)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(pca_2d[:,0], pca_2d[:,1], c=labels, cmap=\"tab10\", s=25, alpha=0.85)\n",
    "plt.title(\"PCA of Autoencoder Latent Space\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../results/ae_pca.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# 5) Reconstruction quality: show original vs reconstructed PSD for a few samples\n",
    "num_examples = min(4, len(X_flat))\n",
    "example_idx = [0, 1, 2, 3][:num_examples]\n",
    "\n",
    "with torch.no_grad():\n",
    "    recon_all, _ = ae_model(X_tensor_flat.to(device))\n",
    "recon_all = recon_all.cpu().numpy()     # shape: (n_samples, input_dim)\n",
    "\n",
    "# For plotting, reshape flattened vectors back to (n_channels, n_freq_bins)\n",
    "n_channels = X.shape[1]\n",
    "n_freq_bins = X.shape[2]\n",
    "\n",
    "plt.figure(figsize=(12, 3 * num_examples))\n",
    "for i, idx in enumerate(example_idx):\n",
    "    orig = X_flat[idx].reshape(n_channels, n_freq_bins)\n",
    "    recon = recon_all[idx].reshape(n_channels, n_freq_bins)\n",
    "\n",
    "    # plot channel-averaged PSD (mean across channels) to reduce clutter\n",
    "    orig_mean = orig.mean(axis=0)\n",
    "    recon_mean = recon.mean(axis=0)\n",
    "\n",
    "    ax = plt.subplot(num_examples, 1, i+1)\n",
    "    ax.plot(orig_mean, label=\"Original (mean across channels)\", linewidth=1)\n",
    "    ax.plot(recon_mean, label=\"Reconstructed (mean across channels)\", linewidth=1, linestyle=\"--\")\n",
    "    ax.set_title(f\"Sample {idx} ‚Äî Original vs Reconstructed (mean PSD)\")\n",
    "    ax.set_xlabel(\"Frequency bin index\")\n",
    "    ax.set_ylabel(\"Log PSD (a.u.)\")\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../results/reconstruction_examples.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# 6) Reconstruction error per sample (to find outliers)\n",
    "recon_error = np.mean((X_flat - recon_all)**2, axis=1)   # MSE per sample\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.hist(recon_error, bins=40)\n",
    "plt.title(\"Histogram of Reconstruction Error (MSE) per Sample\")\n",
    "plt.xlabel(\"MSE\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../results/reconstruction_error_hist.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# 7) Save latent embeddings and reconstruction errors for downstream analysis\n",
    "np.save(\"../results/latent_vectors.npy\", latent)            # shape: (n_samples, latent_dim)\n",
    "np.save(\"../results/reconstruction_error.npy\", recon_error) # shape: (n_samples,)\n",
    "print(\"‚úÖ Saved latent vectors and reconstruction errors to ../results/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
